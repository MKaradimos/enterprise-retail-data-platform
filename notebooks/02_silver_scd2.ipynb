{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥ˆ RetailNova - Notebook 02: Silver Transformation & SCD Type 2\n",
    "\n",
    "This notebook covers:\n",
    "1. Silver transformation logic (cleaning, standardisation)\n",
    "2. SCD Type 2 for customer address changes\n",
    "3. Data Quality checks against Silver tables\n",
    "4. PII masking demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan')\n",
    "\n",
    "from pipelines.spark_session import build_spark_session\n",
    "spark = build_spark_session('RetailNova-Notebook-Silver')\n",
    "print('âœ“ Spark ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Run Silver Transformation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pipelines.silver_transformation import run_silver_transformation\n",
    "run_silver_transformation(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inspect Silver Customers (SCD2 columns) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pipelines.config import storage_config\n",
    "\n",
    "df_silver_cust = spark.read.format('delta').load(\n",
    "    storage_config.layer_path('silver', 'customers')\n",
    ")\n",
    "\n",
    "print(f'Silver Customers count: {df_silver_cust.count()}')\n",
    "\n",
    "# Show SCD2 columns if they exist\n",
    "scd2_cols = ['customer_id', 'email', 'city', 'loyalty_tier',\n",
    "             'effective_start_date', 'effective_end_date', 'is_current']\n",
    "available = [c for c in scd2_cols if c in df_silver_cust.columns]\n",
    "df_silver_cust.select(available).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Simulate an SCD2 Change: Customer changes address â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# In production: this happens automatically via CDC from SQL Server\n",
    "# Here we simulate it manually\n",
    "\n",
    "import psycopg2\n",
    "from pipelines.config import source_config\n",
    "import pyodbc  # or just show the SQL that would run\n",
    "\n",
    "print('In production, a customer address update would look like:')\n",
    "print('''\n",
    "UPDATE dbo.customers\n",
    "SET address_line1 = 'Syntagma 99',\n",
    "    city          = 'Piraeus',\n",
    "    last_modified  = GETDATE()\n",
    "WHERE customer_id = 1;\n",
    "''')\n",
    "print('When the pipeline re-runs:')\n",
    "print('  1. Bronze: new record appended (watermark > previous run)')\n",
    "print('  2. Silver: SCD2 MERGE detects address_line1 change')\n",
    "print('  3. Old row: effective_end_date = NOW(), is_current = FALSE')\n",
    "print('  4. New row: inserted with is_current = TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Silver Products: check margin calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_products = spark.read.format('delta').load(\n",
    "    storage_config.layer_path('silver', 'products')\n",
    ")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print('Product margin analysis:')\n",
    "df_products.select(\n",
    "    'product_name', 'category', 'unit_price', 'cost_price', 'margin_pct'\n",
    ").orderBy(F.col('margin_pct').desc()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Silver Orders: check suspicious flag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_orders = spark.read.format('delta').load(\n",
    "    storage_config.layer_path('silver', 'sales_orders')\n",
    ")\n",
    "\n",
    "print(f'Total orders in Silver: {df_orders.count()}')\n",
    "if 'is_suspicious' in df_orders.columns:\n",
    "    suspicious = df_orders.filter(F.col('is_suspicious') == True)\n",
    "    print(f'Suspicious orders detected: {suspicious.count()}')\n",
    "    suspicious.select('order_id','order_number','status','total_amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Run Data Quality Checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from quality_framework.dq_engine import run_quality_checks\n",
    "\n",
    "results = run_quality_checks(\n",
    "    spark,\n",
    "    layer='silver',\n",
    "    pipeline_name='notebook_quality_check',\n",
    "    raise_on_critical=False\n",
    ")\n",
    "\n",
    "print(f'\\nTotal rules checked: {len(results)}')\n",
    "print(f'PASS:    {sum(1 for r in results if r.status == \"PASS\")}')\n",
    "print(f'WARNING: {sum(1 for r in results if r.status == \"WARNING\")}')\n",
    "print(f'FAIL:    {sum(1 for r in results if r.status == \"FAIL\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inject corrupt data and re-run quality (negative test demo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from quality_framework.dq_engine import inject_corrupt_data\n",
    "\n",
    "print('Injecting 3 corrupt customer records...')\n",
    "inject_corrupt_data(spark, 'customers', 'notebook-test-001')\n",
    "\n",
    "print('Re-running quality checks (expect failures)...')\n",
    "results_after = run_quality_checks(\n",
    "    spark,\n",
    "    layer='silver',\n",
    "    pipeline_name='notebook_corrupt_test',\n",
    "    raise_on_critical=False\n",
    ")\n",
    "\n",
    "failures = [r for r in results_after if r.status == 'FAIL']\n",
    "print(f'\\nCritical failures detected: {len(failures)}')\n",
    "for f in failures:\n",
    "    print(f'  âœ— {f.rule_name}: {f.pass_rate_pct:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.11.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
