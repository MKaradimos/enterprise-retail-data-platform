{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ­ RetailNova - Enterprise Data Platform\n",
    "## Notebook 01: Bronze Layer Ingestion Demo\n",
    "\n",
    "This notebook demonstrates the full Bronze ingestion pipeline:\n",
    "1. Connect to SQL Server (on-prem simulation)\n",
    "2. Read watermark for incremental extraction\n",
    "3. Extract changed records\n",
    "4. Add audit columns\n",
    "5. Write to MinIO (Delta format)\n",
    "6. Update watermark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan')\n",
    "\n",
    "from pipelines.spark_session import build_spark_session\n",
    "from pipelines.config import source_config, storage_config, metadata_config\n",
    "\n",
    "# Build SparkSession (connects to Docker Spark cluster)\n",
    "spark = build_spark_session('RetailNova-Notebook-Bronze')\n",
    "print('âœ“ Spark session created')\n",
    "print(f'  Spark version: {spark.version}')\n",
    "print(f'  Master: {spark.sparkContext.master}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 1: Check what's in the source SQL Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pipelines.config import source_config\n",
    "\n",
    "df_customers = (\n",
    "    spark.read.format('jdbc')\n",
    "    .option('url',      source_config.jdbc_url)\n",
    "    .option('dbtable',  'dbo.customers')\n",
    "    .option('user',     source_config.username)\n",
    "    .option('password', source_config.password)\n",
    "    .option('driver',   source_config.jdbc_properties['driver'])\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(f'Customers in SQL Server: {df_customers.count()}')\n",
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 2: Check watermark state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import psycopg2\n",
    "from pipelines.config import metadata_config\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=metadata_config.host,\n",
    "    port=metadata_config.port,\n",
    "    dbname=metadata_config.database,\n",
    "    user=metadata_config.username,\n",
    "    password=metadata_config.password,\n",
    ")\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute('SELECT * FROM pipeline_watermarks')\n",
    "    print('Current watermarks:')\n",
    "    for row in cur.fetchall():\n",
    "        print(f'  {row[0]:30} -> last_watermark={row[1]}')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 3: Run Bronze Ingestion for a single table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pipelines.bronze_ingestion import run_bronze_ingestion\n",
    "\n",
    "# Run only for 'customers' table first\n",
    "run_bronze_ingestion(spark, tables=['customers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 4: Read back from Bronze Delta â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pipelines.config import storage_config\n",
    "\n",
    "bronze_path = storage_config.layer_path('bronze', 'customers')\n",
    "print(f'Reading from: {bronze_path}')\n",
    "\n",
    "df_bronze = spark.read.format('delta').load(bronze_path)\n",
    "print(f'Bronze customers count: {df_bronze.count()}')\n",
    "df_bronze.printSchema()\n",
    "df_bronze.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 5: Show Delta table history (time travel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forPath(spark, bronze_path)\n",
    "dt.history().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 6: Run ALL tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "run_bronze_ingestion(spark)  # No tables param = runs all\n",
    "print('\\nâœ“ All Bronze tables loaded')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.11.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
