# ðŸ­ RetailNova Enterprise Data Platform

> **Production-ready cloud data engineering platform** built with Azure-equivalent technologies.
> Simulates a real consulting delivery for a retail company migrating from on-prem SQL Server to a cloud data lake.

---

## ðŸ“‹ Executive Summary

RetailNova is a Greek retail chain operating 4 physical stores and an online channel.
Their legacy setup â€” on-prem SQL Server + manual Excel reporting â€” could not support
growth, lacked data governance, and had zero automation.

This platform delivers:
- **Automated daily ingestion** from SQL Server via incremental CDC
- **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) in Delta Lake format
- **Star Schema** dimensional model for analytics
- **Data Quality Framework** with 20+ configurable rules
- **Real-time monitoring** dashboard via Grafana
- **Unit test suite** covering all transformation logic

---

## ðŸŽ¯ Business Objectives

| Objective | Solution |
|-----------|----------|
| Replace manual Excel reporting | Gold layer + Grafana/Power BI |
| Automate daily data refresh | Master pipeline with retry logic |
| Ensure data accuracy | DQ Framework with configurable thresholds |
| Cloud scalability | Delta Lake on object storage (S3/ADLS) |
| Enable future AI/ML | Feature-ready Gold layer (RFM, CLV, cohorts) |
| GDPR compliance | PII masking at Silver layer + audit logs |

---

## ðŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOURCE LAYER                                                â”‚
â”‚  SQL Server (RetailNova_OLTP)                               â”‚
â”‚  Tables: customers, products, stores, orders, order_lines   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ JDBC Incremental (watermark CDC)
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER  (Raw / Immutable)                             â”‚
â”‚  Format: Delta | Partition: year/month                      â”‚
â”‚  Mode: APPEND only (never overwrite)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Clean + Validate + SCD2
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER  (Cleaned / Business-validated)                â”‚
â”‚  Format: Delta | Mode: MERGE (upsert)                       â”‚
â”‚  SCD2 customers | PII masked | DQ validated                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Star Schema + KPIs + Aggregations
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD LAYER  (Business-Ready)                                â”‚
â”‚  dim_customer | dim_product | dim_store | dim_date          â”‚
â”‚  fact_sales | agg_monthly_kpis | cohort_analysis            â”‚
â”‚  customer_segments (RFM)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              Power BI / Grafana Dashboards
```

### Local Stack (Docker)
| Component | Azure Equivalent | Port |
|-----------|-----------------|------|
| SQL Server 2022 | Azure SQL Database | 1433 |
| MinIO | Azure Data Lake Gen2 | 9000/9001 |
| Apache Spark 3.5 | Azure Databricks | 7077/8080 |
| Jupyter Lab | Databricks Notebooks | 8888 |
| PostgreSQL | Azure SQL (metadata) | 5432 |
| Grafana | Power BI / Azure Monitor | 3000 |

---

## ðŸš€ Quick Start

### Prerequisites
- Docker Desktop (4GB RAM minimum)
- Python 3.9+
- Git

### 1. Clone & Setup
```bash
git clone https://github.com/your-username/enterprise-retail-data-platform
cd enterprise-retail-data-platform

# Windows
setup_windows.bat

# macOS / Linux
chmod +x setup.sh && ./setup.sh
```

### 2. Run the Full Pipeline
```bash
python run_pipeline.py --layer all
```

### 3. Run Tests
```bash
python run_pipeline.py --tests
# or
python -m pytest tests/test_pipeline.py -v
```

### 4. Open Jupyter (interactive)
```
http://localhost:8888   (token: retailnova2024)

Notebooks:
  01_bronze_ingestion.ipynb  - Bronze CDC demo
  02_silver_scd2.ipynb       - SCD2 + DQ checks
  03_gold_analytics.ipynb    - Star schema + KPIs
```

### 5. Monitor in Grafana
```
http://localhost:3000   (admin / RetailNova@2024)
Dashboard: "RetailNova - Pipeline Operations Dashboard"
```

---

## ðŸ“ Project Structure

```
enterprise-retail-data-platform/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # All services
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ sql/01_seed_oltp.sql         # SQL Server source data
â”‚   â””â”€â”€ postgres/01_init_metadata.sql# Metadata DB schema + DQ rules
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ config.py                    # Central configuration
â”‚   â”œâ”€â”€ spark_session.py             # SparkSession builder
â”‚   â”œâ”€â”€ logger.py                    # Structured pipeline logging
â”‚   â”œâ”€â”€ bronze_ingestion.py          # Layer 1: Raw extraction
â”‚   â”œâ”€â”€ silver_transformation.py     # Layer 2: Clean + SCD2
â”‚   â”œâ”€â”€ gold_pipeline.py             # Layer 3: Star schema + KPIs
â”‚   â””â”€â”€ master_pipeline.py           # Orchestrator with retry/alerts
â”œâ”€â”€ quality_framework/
â”‚   â””â”€â”€ dq_engine.py                 # Generic DQ rule engine
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_silver_scd2.ipynb
â”‚   â””â”€â”€ 03_gold_analytics.ipynb
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ grafana/
â”‚       â”œâ”€â”€ datasources/             # Auto-configured PostgreSQL
â”‚       â””â”€â”€ dashboards/              # Pre-built operations dashboard
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py             # 25+ unit tests
â”œâ”€â”€ documentation/
â”‚   â””â”€â”€ architecture.md              # Design decisions
â”œâ”€â”€ run_pipeline.py                  # CLI entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.sh                         # macOS/Linux setup
â””â”€â”€ setup_windows.bat                # Windows setup
```

---

## ðŸ”„ Data Flow & Patterns

### Incremental Load (CDC)
```python
# Watermark-based: only extract rows changed since last run
WHERE last_modified > '2024-11-30 00:00:00'  # from watermarks table

# After success: update watermark to NOW()
UPDATE pipeline_watermarks SET last_watermark = NOW() WHERE table_name = 'customers'
```

### SCD Type 2 (Customers)
```
Customer 1 changes city: Athens â†’ Piraeus

Before:
  customer_id=1, city=Athens, is_current=TRUE

After MERGE:
  customer_id=1, city=Athens,  effective_end=2024-06-15, is_current=FALSE
  customer_id=1, city=Piraeus, effective_end=NULL,       is_current=TRUE
```

### Data Quality Rules (from DB)
```sql
-- Rules stored in data_quality_rules table
-- Engine reads rules dynamically and executes them
SELECT * FROM data_quality_rules WHERE is_active = TRUE;

-- Results written to data_quality_log
SELECT * FROM data_quality_log ORDER BY run_at DESC LIMIT 20;
```

---

## ðŸ“Š KPIs Computed

| KPI | Description |
|-----|-------------|
| **Total Revenue** | Sum of line amounts for Delivered/Shipped orders |
| **Average Order Value (AOV)** | Revenue / Order Count |
| **Basket Size** | Avg items per order |
| **Unique Customers** | Monthly distinct buyers |
| **Customer Lifetime Value (CLV)** | Avg total spend per customer |
| **Repeat Purchase Rate** | % customers with 2+ orders |
| **RFM Segments** | Champions, Loyal, At-Risk, Lost, New |
| **Cohort Retention** | Month-0 through Month-N retention by acquisition cohort |

---

## ðŸ›¡ Data Governance

### PII Handling
- **Silver layer**: email and phone SHA-256 hashed
- **Gold layer**: no direct PII (aggregate/segment only)
- **Audit trail**: every pipeline run logged with run_id

### Access Control (Azure RBAC simulation)
```
Bronze  â†’ data_engineers group only
Silver  â†’ data_engineers (write), data_analysts (read)
Gold    â†’ data_engineers (write), bi_users (read)
```

### GDPR Right to Erasure
```sql
-- Soft-delete customer (no hard deletes in Delta)
UPDATE dbo.customers SET is_active = FALSE WHERE customer_id = ?

-- Next pipeline run: SCD2 expires the record
-- Masked data in Silver means no recoverable PII
```

---

## âš™ï¸ Pipeline CLI Reference

```bash
# Full end-to-end pipeline
python run_pipeline.py --layer all

# Individual layers
python run_pipeline.py --layer bronze
python run_pipeline.py --layer silver
python run_pipeline.py --layer gold
python run_pipeline.py --layer quality

# Specific tables only (bronze)
python run_pipeline.py --layer bronze --tables customers products

# Stop on first failure
python run_pipeline.py --layer all --fail-fast

# Skip DQ checks (faster dev iteration)
python run_pipeline.py --layer all --skip-quality

# Check service connectivity
python run_pipeline.py --status

# Run test suite
python run_pipeline.py --tests
```

---

## ðŸ§ª Test Coverage

```
tests/test_pipeline.py - 25 unit tests

TestSilverCustomerTransformation  (7 tests)
  âœ“ email lowercase and trim
  âœ“ first name initcap
  âœ“ country uppercase
  âœ“ age calculation
  âœ“ email regex valid
  âœ“ email regex invalid
  âœ“ deduplication keeps latest record

TestSilverProductTransformation   (3 tests)
  âœ“ product code uppercase
  âœ“ product name trimmed
  âœ“ zero price filtered

TestSilverOrderTransformation     (3 tests)
  âœ“ negative shipping cost zeroed
  âœ“ discount > 100% zeroed
  âœ“ suspicious order flag

TestDataQualityRules              (6 tests)
  âœ“ not_null PASS / FAIL
  âœ“ unique PASS / FAIL
  âœ“ range PASS / FAIL
  âœ“ completeness threshold

TestSchemaValidation              (3 tests)
  âœ“ required columns present
  âœ“ missing column detected
  âœ“ extra columns allowed

TestRowCountReconciliation        (2 tests)
  âœ“ bronze-to-silver count
  âœ“ deduplication reduces count

TestNegativeScenarios             (4 tests)
  âœ“ null email caught by DQ
  âœ“ negative quantity filtered
  âœ“ corrupt file schema mismatch
  âœ“ order total mismatch detection
  âœ“ SCD2 address change detection
```

---

## ðŸ§  Interview Q&A Guide

**Q: How did you implement SCD Type 2?**
> Delta MERGE with change detection condition. When tracked columns (address, email, loyalty_tier) differ between source and target, the old row gets `effective_end_date = NOW()` and `is_current = FALSE`. A new row is inserted by `whenNotMatchedInsertAll()`. Non-tracked column updates (name, phone) use `whenMatchedUpdate()` without expiring the row.

**Q: How does your incremental loading work?**
> Watermark table in PostgreSQL stores the `last_modified` timestamp per source table. Each run reads `WHERE last_modified > watermark`, processes the data, then updates the watermark to `NOW()`. This is a high-water mark pattern. For production we'd upgrade to Debezium CDC â†’ Event Hubs â†’ Auto Loader.

**Q: How did you design the data quality framework?**
> Rules are stored as rows in `data_quality_rules` table (rule_type, threshold, severity). The DQ engine loads all active rules, dispatches each to a type-specific executor function, captures pass/fail rates, and writes results to `data_quality_log`. Critical failures trigger alerts. This makes it metadata-driven â€” adding a new rule needs only a DB INSERT, not a code change.

**Q: How do you handle pipeline failures?**
> Master pipeline uses a retry wrapper with exponential backoff (delay * 2^attempt). Each child pipeline runs in its own `pipeline_run()` context manager which logs start/end/duration/rows. If Bronze fails, Silver is automatically skipped (conditional branching). Alerts are sent on failure and SLA breach. All errors are logged to `error_log` with stack traces.

**Q: How would you scale this to 100x data?**
> Switch from JDBC polling to streaming CDC (Debezium â†’ Event Hubs â†’ Auto Loader). Increase `spark.sql.shuffle.partitions` from 4 to 400+. Add Z-ORDER indexing on `fact_sales(order_date, customer_id)`. Use Delta Liquid Clustering for auto-layout. Separate Gold and Silver job clusters with different SLAs. Consider Databricks Photon engine for Gold aggregations.

---

## ðŸ“ˆ Scalability Roadmap

| Phase | Feature | Value |
|-------|---------|-------|
| Current | Batch, daily | âœ“ Done |
| Phase 2 | Streaming (5-min micro-batch) | Near-real-time analytics |
| Phase 3 | Feature Store + Azure ML | Churn prediction, price optimisation |
| Phase 4 | Data Mesh | Decentralised domain ownership |

---

## ðŸŽ“ Lessons Learned

1. **Delta MERGE is powerful but has edge cases** â€” always test SCD2 merge conditions with duplicate keys before production
2. **Watermark updates must be atomic** â€” if the pipeline fails after write but before watermark update, you'll re-process data â†’ ensure idempotent writes (Delta MERGE handles this)
3. **Shuffle partitions for local dev** â€” the default 200 partitions is fine for prod but crushes local performance; set to 2-4 for dev
4. **DQ rules in DB, not code** â€” code changes require deployment; DB changes can be hotfixed without deployment
5. **Test negative scenarios explicitly** â€” corrupt data injection is the only way to verify your DQ rules actually fire

---

*Built as a consulting case study for RetailNova Analytics.
Demonstrates production-level Azure data engineering patterns.*
